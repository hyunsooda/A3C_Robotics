#!/usr/bin/env python

import rospy
import os
import json
import numpy as np
import random
import time
import sys
import threading
import socket
import json
from multiprocessing import Process
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))
from collections import deque
from std_msgs.msg import Float32MultiArray, Float32, Int16, Bool
from src.turtlebot3_dqn.environment_stage_1 import Env
from keras.models import Sequential, load_model
from keras.optimizers import RMSprop
from keras.layers import Dense, Dropout, Activation, Input 
from keras.models import Model
from keras import backend as K


EPISODES = 3000
rospy.init_node('turtlebot3_a3c_global')
pub_result = rospy.Publisher('result', Float32MultiArray, queue_size=5)
pub_get_action = rospy.Publisher('get_action', Float32MultiArray, queue_size=5)
result = Float32MultiArray()
get_action = Float32MultiArray()
lock = threading.Lock()

class A3CAgent:
    def __init__(self, state_size, action_size):
        self.dirPath = os.path.dirname(os.path.realpath(__file__))
        self.dirPath = self.dirPath.replace('turtlebot3_dqn/nodes', 'turtlebot3_dqn/save_model/stage_1_')
        
        self.load_model = False
        self.load_episode = 0
        self.state_size = state_size
        self.action_size = action_size
        self.episode_step = 6000
        self.target_update = 2000
        self.discount_factor = 0.99
        self.learning_rate = 0.00025
        self.epsilon = 1.0
        self.epsilon_decay = 0.99
        self.epsilon_min = 0.05
        self.batch_size = 64
        self.train_start = 64
        self.states, self.actions = [],[]
        self.advantages = 0.0
        self.discounted_prediction = 0.0
        self.count = 0
        self.whoami = False
        self.trigger_update = False

        self.actor, self.critic = self.buildModel()
        self.optimizer = [self.actor_optimizer(), self.critic_optimizer()]

        if self.load_model:
            self.loadModel()

    def loadModel(self):
        self.actor.load_weights(self.dirPath + "_actor.h5")
        self.critic.load_weights(self.dirPath + "_critic.h5")

    def saveModel(self):
        self.actor.save_weights(self.dirPath + "_actor.h5")
        self.critic.save_weights(self.dirPath + "_critic.h5")

    def buildModel(self):
        input = Input(shape=(self.state_size,))
        dropout = 0.2
        fc1 = Dense(64, activation='relu', kernel_initializer='lecun_uniform')(input)
        fc2 = Dense(64, activation='relu', kernel_initializer='lecun_uniform')(fc1)
        fc3 = Dropout(dropout)(fc2)
        
        policy = Dense(self.action_size, activation='softmax')(fc3)
        value = Dense(1, activation='linear')(fc3)

        actor = Model(inputs=input, outputs=policy)
        critic = Model(inputs=input, outputs=value)

        actor._make_predict_function()
        critic._make_predict_function()

        actor.summary()
        critic.summary()
        
        return actor, critic

    def actor_optimizer(self):
        action = K.placeholder(shape=[None, self.action_size])
        advantages = K.placeholder(shape=(None, ))

        policy = self.actor.output

        action_prob = K.sum(action * policy, axis=1)
        cross_entropy = K.log(action_prob + 1e-10) * advantages
        cross_entropy = -K.sum(cross_entropy)

        entropy = K.sum(policy * K.log(policy + 1e-10), axis=1)
        entropy = K.sum(entropy)

        loss = cross_entropy + 0.01 * entropy

        optimizer = RMSprop(lr=self.learning_rate, rho=0.99, epsilon=0.01)
        updates = optimizer.get_updates(self.actor.trainable_weights, [],loss)
        train = K.function([self.actor.input, action, advantages],
                           [loss], updates=updates)
        return train

    def critic_optimizer(self):
        discounted_prediction = K.placeholder(shape=(None,))

        value = self.critic.output

        loss = K.mean(K.square(discounted_prediction - value))

        optimizer = RMSprop(lr=self.learning_rate, rho=0.99, epsilon=0.01)
        updates = optimizer.get_updates(self.critic.trainable_weights, [],loss)
        train = K.function([self.critic.input, discounted_prediction],
                           [loss], updates=updates)
        return train

    def get_samples(self):
        while True:
            (conn, address) = self.server.accept()
            t = threading.Thread(target=self.marshaling, args=(conn, address))
            t.start()
            
    def run(self):
        while True:
            with lock:
                if self.trigger_update == True:
                    self.optimizer[0]([self.states, self.actions, self.advantages])
                    self.optimizer[1]([self.states, self.discounted_prediction])
                    self.saveModel()

                    self.conn.send(str(self.whoami))
                    if self.whoami != False:
                        print("Sended to local network ", self.whoami)
                        self.whoami = False    

                    self.trigger_update = False

    def marshaling(self, conn, address):
        while True:
            data = conn.recv(1000000)
            data = json.loads(data)
            
            with lock:
                self.states = np.array(data[0]).reshape(-1, 26)
                self.actions = np.array(data[1]).reshape(-1, 5)
                self.advantages = np.array(data[2])
                self.discounted_prediction = np.array(data[3])
                self.whoami = data[4]
                self.conn = conn     
                self.trigger_update = True     


if __name__ == '__main__':
    global_agent = A3CAgent(state_size=26, action_size=5)
    global_agent.server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    global_agent.server.bind(('127.0.0.1', 5541))
    global_agent.server.listen(5)
    t = threading.Thread(target=global_agent.get_samples)
    t.start()
    global_agent.run()
    # global_agent.states = np.zeros(26 * 10)
    # global_agent.actions = np.zeros(5 * 10)
    # global_agent.advantages = np.zeros(1 * 10)
    # global_agent.discounted_prediction = np.zeros(1)

    # global_agent.optimizer[0]([global_agent.states.reshape(-1, 26), global_agent.actions.reshape(-1,5), global_agent.advantages] )
    # global_agent.optimizer[1]([global_agent.states.reshape(1,26), global_agent.discounted_prediction.reshape(1,1)])
    # global_agent.actor.predict(global_agent.states.reshape(1, 26))
